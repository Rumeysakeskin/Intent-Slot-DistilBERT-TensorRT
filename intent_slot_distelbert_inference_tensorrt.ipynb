{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMVm1qhbZ2ks3rlS+XKGr7o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rumeysakeskin/NLP-Onnx-TensorRT/blob/main/intent_slot_distelbert_inference_tensorrt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pycuda\n",
        "!pip install nvidia-pyindex\n",
        "!pip install nvidia-tensorrt\n",
        "!pip install transformers\n",
        "!pip install omegaconf\n",
        "!pip install ruamel-yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LAskePU-EUCB",
        "outputId": "c785cb20-e9a1-459e-d7d0-975c75be15aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pycuda\n",
            "  Downloading pycuda-2022.2.2.tar.gz (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pytools>=2011.2 (from pycuda)\n",
            "  Downloading pytools-2023.1-py2.py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: appdirs>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from pycuda) (1.4.4)\n",
            "Collecting mako (from pycuda)\n",
            "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from pytools>=2011.2->pycuda) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pytools>=2011.2->pycuda) (4.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from mako->pycuda) (2.1.2)\n",
            "Building wheels for collected packages: pycuda\n",
            "  Building wheel for pycuda (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycuda: filename=pycuda-2022.2.2-cp310-cp310-linux_x86_64.whl size=661975 sha256=b58a8f74cb559964dd89f5a300dbc6a8920e1bf37bc73345f2be70083656ea34\n",
            "  Stored in directory: /root/.cache/pip/wheels/1d/7b/06/82a395a243fce00035dea9914d92bbef0013401497d849f8bc\n",
            "Successfully built pycuda\n",
            "Installing collected packages: pytools, mako, pycuda\n",
            "Successfully installed mako-1.2.4 pycuda-2022.2.2 pytools-2023.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nvidia-pyindex\n",
            "  Downloading nvidia-pyindex-1.0.9.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: nvidia-pyindex\n",
            "  Building wheel for nvidia-pyindex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nvidia-pyindex: filename=nvidia_pyindex-1.0.9-py3-none-any.whl size=8418 sha256=e148d4fdd33613b59a8df2698cd35c3b6178ff8f168eb64fe31135457ad8cbfd\n",
            "  Stored in directory: /root/.cache/pip/wheels/2c/af/d0/7a12f82cab69f65d51107f48bcd6179e29b9a69a90546332b3\n",
            "Successfully built nvidia-pyindex\n",
            "Installing collected packages: nvidia-pyindex\n",
            "Successfully installed nvidia-pyindex-1.0.9\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://pypi.ngc.nvidia.com\n",
            "Collecting nvidia-tensorrt\n",
            "  Downloading nvidia_tensorrt-99.0.0-py3-none-manylinux_2_17_x86_64.whl (17 kB)\n",
            "Collecting tensorrt (from nvidia-tensorrt)\n",
            "  Downloading tensorrt-8.6.1.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: tensorrt\n",
            "  Building wheel for tensorrt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tensorrt: filename=tensorrt-8.6.1-py2.py3-none-any.whl size=16973 sha256=ff242f15c879fa15941038c1686d99a229f5c1ddfcb6d17aa2d12ed6e79faa62\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/29/56/abdffd4c604f255b5254bef3f1c598ab7811ea020540599438\n",
            "Successfully built tensorrt\n",
            "Installing collected packages: tensorrt, nvidia-tensorrt\n",
            "Successfully installed nvidia-tensorrt-99.0.0 tensorrt-8.6.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://pypi.ngc.nvidia.com\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m110.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.15.1 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://pypi.ngc.nvidia.com\n",
            "Collecting omegaconf\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting antlr4-python3-runtime==4.9.* (from omegaconf)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from omegaconf) (6.0)\n",
            "Building wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=5b43c9c42fafdb37463d19d4f3981b25486457dd3ad832c61044dcfbe3c3396f\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, omegaconf\n",
            "Successfully installed antlr4-python3-runtime-4.9.3 omegaconf-2.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://pypi.ngc.nvidia.com\n",
            "Collecting ruamel-yaml\n",
            "  Downloading ruamel.yaml-0.17.32-py3-none-any.whl (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.2/112.2 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ruamel.yaml.clib>=0.2.7 (from ruamel-yaml)\n",
            "  Downloading ruamel.yaml.clib-0.2.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (485 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.6/485.6 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ruamel.yaml.clib, ruamel-yaml\n",
            "Successfully installed ruamel-yaml-0.17.32 ruamel.yaml.clib-0.2.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9TSkwutDf0d",
        "outputId": "04cf8ef8-c2ee-4b69-b336-f1894f0f5a63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "binding 0 - 'input_ids'\n",
            "   input:    True\n",
            "   shape:    (1, 50)\n",
            "   dtype:    DataType.INT32\n",
            "   size:     200\n",
            "   dynamic:  True\n",
            "   profiles: [{'min': (1, 50), 'opt': (1, 50), 'max': (1, 50)}]\n",
            "\n",
            "\n",
            "binding 1 - 'attention_mask'\n",
            "   input:    True\n",
            "   shape:    (1, 50)\n",
            "   dtype:    DataType.INT32\n",
            "   size:     200\n",
            "   dynamic:  True\n",
            "   profiles: [{'min': (1, 50), 'opt': (1, 50), 'max': (1, 50)}]\n",
            "\n",
            "\n",
            "binding 2 - 'intent_logits'\n",
            "   input:    False\n",
            "   shape:    (1, 12)\n",
            "   dtype:    DataType.FLOAT\n",
            "   size:     48\n",
            "   dynamic:  True\n",
            "   profiles: []\n",
            "\n",
            "\n",
            "binding 3 - 'slot_logits'\n",
            "   input:    False\n",
            "   shape:    (1, 50, 11)\n",
            "   dtype:    DataType.FLOAT\n",
            "   size:     2200\n",
            "   dynamic:  True\n",
            "   profiles: []\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from jetson_voice_utils.trt_model import TRTModel\n",
        "from transformers import AutoTokenizer\n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "# from ruamel.yaml import YAML\n",
        "# from omegaconf import DictConfig\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "class ENNemoDialogueInferencerTRT(object):\n",
        "    def __init__(self):\n",
        "\n",
        "        config_file = \"confing/intent_slot_jetson.yaml\"\n",
        "\n",
        "        self.en_config = OmegaConf.load(config_file)\n",
        "\n",
        "        self.en_config.model_path = \"ENGLISH_nemo_model_DISTEL_BERT.onnx\" #ENGLISH\n",
        "\n",
        "        self.intent_slot_label_path = \"/nemo_format/\" #ENGLISH\n",
        "\n",
        "        self.nlp_dynamic_shapes = False\n",
        "        dynamic_shapes = {'max' : (1, self.en_config.model['language_model']['max_seq_length'])}  # (batch_size, sequence)\n",
        "        if self.nlp_dynamic_shapes:\n",
        "            dynamic_shapes['min'] = (1, 1)\n",
        "\n",
        "\n",
        "        self.en_TRT_model = TRTModel(self.en_config, dynamic_shapes)\n",
        "\n",
        "        # create tokenizer\n",
        "        self.en_tokenizer = AutoTokenizer.from_pretrained(self.en_config.model['tokenizer']['tokenizer_name'])\n",
        "        #self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "        #self.tokenizer.add_special_tokens({'sep_token': '[SEP]'})\n",
        "\n",
        "        self.null_slot = self.slot_labels()[-1]  # 'O' in assistant dataset - always the last label?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def to_numpy(self, tensor):\n",
        "        return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
        "\n",
        "    def normalize_logits(self, logits):\n",
        "        \"\"\"\n",
        "        Normalize logits such that they are distributed between [0,1]\n",
        "        \"\"\"\n",
        "        return np.exp(logits - np.log(np.sum(np.exp(logits), axis=-1, keepdims=True)))\n",
        "\n",
        "    def intent_labels(self):\n",
        "        \"\"\"\n",
        "        List of the intent class labels.\n",
        "        \"\"\"\n",
        "        # Define intent label map\n",
        "        intent_labels = []\n",
        "        with open(self.intent_slot_label_path + 'dict.intents.csv', 'r') as file:\n",
        "            for intent in file:\n",
        "                intent_labels.append(intent.strip())\n",
        "\n",
        "        return intent_labels\n",
        "\n",
        "    def intent_label(self, index):\n",
        "        \"\"\"\n",
        "        Return an intent label by index (with bounds checking)\n",
        "        \"\"\"\n",
        "        return self.intent_labels()[int(index)] if index < len(self.intent_labels()) else 'Unknown_Intent'\n",
        "\n",
        "    def slot_labels(self):\n",
        "        \"\"\"\n",
        "        List of the slot class labels.\n",
        "        \"\"\"\n",
        "        # Define slot label map\n",
        "        slot_labels = []\n",
        "        with open(self.intent_slot_label_path + 'dict.slots.csv', 'r') as file:\n",
        "            for slot in file:\n",
        "                slot_labels.append(slot.strip())\n",
        "\n",
        "        return slot_labels\n",
        "\n",
        "    def slot_label(self, index):\n",
        "        \"\"\"\n",
        "        Return a slot label by index (with bounds checking)\n",
        "        \"\"\"\n",
        "        return self.slot_labels()[int(index)] if index < len(self.slot_labels()) else self.null_slot\n",
        "\n",
        "    def find_subtokens(self, encodings, method='char_span'):\n",
        "        \"\"\"\n",
        "        Compute the subtoken mask, where each token is marked as True if it's a subtoken or False otherwise.\n",
        "        Longer words/acronyms may be tokenized into mulitple word pieces (called subtokens), for example:\n",
        "\n",
        "            'Yosemite' -> ['yo', '##se', '##mite']\n",
        "            'U.S.' -> ['u', '.', 's', '.']\n",
        "\n",
        "        Parameters:\n",
        "          encodings (BatchEncoding) -- Output from tokenizer\n",
        "\n",
        "          method (string) -- If 'char_span', the subtoken mask will be determined by looking at the character\n",
        "                             indices.  Tokens that map to characters that are side-by-side are flagged as subtokens.\n",
        "\n",
        "                             If 'subtoken_delimiters', subtokens will be identified by looking for '##' symbols.\n",
        "                             However this can miss punctuated subtokens, such as 'U.S.'\n",
        "\n",
        "        Returns boolean subtoken mask array with shape (num_queries, num_tokens)\n",
        "        \"\"\"\n",
        "        num_queries = encodings['input_ids'].shape[0]\n",
        "        subtoken_mask = []\n",
        "\n",
        "        if method == 'char_span':\n",
        "            for query_idx in range(num_queries):\n",
        "                mask = []\n",
        "                last_char = -1\n",
        "                tokens = encodings.tokens(query_idx)\n",
        "\n",
        "                for token_idx, word_id in enumerate(encodings.word_ids(query_idx)):\n",
        "                    if word_id is None:  # skip special tokens\n",
        "                        mask.append(False)\n",
        "                        continue\n",
        "\n",
        "                    chars = encodings.token_to_chars(query_idx, token_idx)\n",
        "\n",
        "                    if chars[0] == last_char:\n",
        "                        mask.append(True)\n",
        "                    else:\n",
        "                        mask.append(False)\n",
        "\n",
        "                    last_char = chars[1]\n",
        "\n",
        "                subtoken_mask.append(mask)\n",
        "\n",
        "        elif method == 'subtoken_delimiters':\n",
        "            for query_idx in range(num_queries):\n",
        "                subtoken_mask.append([token.startswith('##') for token in encodings.tokens(query_idx)])\n",
        "        else:\n",
        "            raise ValueError(f\"invalid method ('{method}')\")\n",
        "\n",
        "        return np.asarray(subtoken_mask)\n",
        "\n",
        "\n",
        "    def inference(self, queries):\n",
        "\n",
        "        start = time.time()\n",
        "        lang = \"en\"\n",
        "\n",
        "        encodings = getattr(self, f\"{lang}_tokenizer\")(\n",
        "            text=query,\n",
        "            padding='longest' if self.nlp_dynamic_shapes else 'max_length',\n",
        "            truncation=True,\n",
        "            max_length=getattr(self, f\"{lang}_config\").model['language_model']['max_seq_length'],\n",
        "            return_tensors='np',\n",
        "            return_token_type_ids=True,\n",
        "            return_overflowing_tokens=True,\n",
        "            return_offsets_mapping=True,\n",
        "            return_special_tokens_mask=True,\n",
        "        )\n",
        "\n",
        "        # during slot classification, we want to ignore slots from subtokens and special tokens\n",
        "        subtoken_mask = self.find_subtokens(encodings, method='subtoken_delimiters')\n",
        "        ignore_mask = subtoken_mask | encodings['special_tokens_mask']\n",
        "\n",
        "        inputs = {}\n",
        "\n",
        "        for input in getattr(self, f\"{lang}_TRT_model\").inputs:\n",
        "            if input.name not in encodings:\n",
        "                raise ValueError(f\"the encoded inputs from the tokenizer doesn't contain '{input.name}'\")\n",
        "\n",
        "            inputs[input.name] = encodings[input.name]\n",
        "\n",
        "        # run the model\n",
        "        intent_logits, slot_logits = getattr(self, f\"{lang}_TRT_model\").execute(inputs)\n",
        "\n",
        "\n",
        "        intent_logits = self.normalize_logits(intent_logits)\n",
        "        slot_logits = self.normalize_logits(slot_logits)\n",
        "\n",
        "        intent_preds = np.argmax(intent_logits, axis=-1)\n",
        "        slot_preds = np.argmax(slot_logits, axis=-1)\n",
        "\n",
        "\n",
        "        # convert numerical outputs to intent/slot labels\n",
        "        results = []\n",
        "\n",
        "        for query_idx, intent_id in enumerate(intent_preds):\n",
        "            results.append({\n",
        "                'intent' : self.intent_label(intent_id),\n",
        "                'score' : intent_logits[query_idx][intent_id],\n",
        "                #'slots' : []\n",
        "            })\n",
        "\n",
        "        for query_idx, slots in enumerate(slot_preds):\n",
        "            query_slots = [self.slot_label(slot) for slot in slots]\n",
        "\n",
        "\n",
        "            for token_idx, slot in enumerate(query_slots):\n",
        "                # ignore unclassified slots or masked tokens\n",
        "                if slot == self.null_slot or ignore_mask[query_idx][token_idx]:\n",
        "                    continue\n",
        "\n",
        "                # convert from token index back to the query string\n",
        "                chars = encodings.token_to_chars(query_idx, token_idx)\n",
        "\n",
        "                text = query[chars[0]:chars[1]]      # queries[query_idx]\n",
        "\n",
        "                # append subtokens from the query to the text\n",
        "                for subtoken_idx in range(token_idx+1, len(query_slots)):\n",
        "                    if subtoken_mask[query_idx][subtoken_idx]:\n",
        "                        subtoken_chars = encodings.token_to_chars(query_idx, subtoken_idx)\n",
        "\n",
        "                        text += query[subtoken_chars[0]:subtoken_chars[1]]\n",
        "                    else:\n",
        "                        break\n",
        "\n",
        "                results.append({\n",
        "                    'slot' : slot,\n",
        "                    'text' : text,\n",
        "                    'score' : slot_logits[query_idx][token_idx][slots[token_idx]]\n",
        "                })\n",
        "\n",
        "        intent = ''\n",
        "        intent_score = 0.0\n",
        "        merged_data = {}\n",
        "\n",
        "        for item in results:\n",
        "            if 'intent' in item:\n",
        "                intent = item['intent']\n",
        "                intent_score = item['score']\n",
        "            elif 'slot' in item:\n",
        "                slot = item['slot']\n",
        "                text = item['text']\n",
        "                score = item['score']\n",
        "\n",
        "                if slot not in merged_data:\n",
        "                    merged_data[slot] = {'text': [], 'score': 0.0}\n",
        "\n",
        "                merged_data[slot]['text'].append(text)\n",
        "\n",
        "                if score > merged_data[slot]['score']:\n",
        "                    merged_data[slot]['score'] = score\n",
        "\n",
        "        return intent, intent_score, merged_data, time.time()-start\n",
        "\n",
        "en_inference_model = ENNemoDialogueInferencerTRT()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"please prepare two sweet filter coffee with milk\"\n",
        "\n",
        "intent, intent_score, merged_data, inference_time = en_inference_model.inference(query)\n",
        "\n",
        "print(f\"\\nQuery: \\033[1m{query}\\033[0m\\n\")\n",
        "\n",
        "print(f\"Predicted Intent: \\033[1m{intent} (Score: {intent_score:.4f}\\033[0m)\\n\")\n",
        "\n",
        "print(\"Predicted Slot(s):\")\n",
        "for slot, values in merged_data.items():\n",
        "    print(f\"\\033[1m{slot} --> {' '.join(values['text'])} (Score: {values['score']:.4f}\\033[0m)\")\n",
        "\n",
        "print(f\"\\nInference time: \\033[1m{inference_time:.4f} sec\\033[0m\")"
      ],
      "metadata": {
        "id": "vLcIIcBgDk5n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d4c03c9-627d-4ff0-a71e-947608304e89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs: {'input_ids': array([[  101,  7374,  2048,  4086, 11307,  4157,  2007,  6501,   102,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0]]), 'attention_mask': array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0]])}\n",
            "self.inputs: [<jetson_voice_utils.trt_binding.Binding object at 0x7f190e71f700>, <jetson_voice_utils.trt_binding.Binding object at 0x7f18769d3820>]\n",
            "\n",
            "Query: \u001b[1mplease prepare two sweet filter coffee with milk\u001b[0m\n",
            "\n",
            "Predicted Intent: \u001b[1mcoffee_machine_brewing_query (Score: 0.9904\u001b[0m)\n",
            "\n",
            "Predicted Slot(s):\n",
            "\u001b[1mamount_per_person --> two (Score: 0.6788\u001b[0m)\n",
            "\u001b[1mcontent --> sweet with milk (Score: 0.9168\u001b[0m)\n",
            "\u001b[1mcoffee_type --> filter coffee (Score: 0.9348\u001b[0m)\n",
            "\n",
            "Inference time: \u001b[1m0.0100 sec\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}