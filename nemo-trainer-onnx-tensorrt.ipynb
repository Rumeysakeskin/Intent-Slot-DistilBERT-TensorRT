{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c7a292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install NeMo\n",
    "BRANCH = 'main'\n",
    "!python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[nlp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2275bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install editdistance\n",
    "!pip install --upgrade onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0666c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-05-31 05:54:47 optimizers:66] Could not import distributed_fused_adam optimizer from Apex\n",
      "[NeMo W 2023-05-31 05:54:48 experimental:27] Module <class 'nemo.collections.nlp.models.text_normalization_as_tagging.thutmose_tagger.ThutmoseTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-05-31 05:54:49 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "      warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from nemo.collections import nlp as nemo_nlp\n",
    "from nemo.utils.exp_manager import exp_manager\n",
    "from nemo.utils import logging\n",
    "import os\n",
    "import wget\n",
    "import pytorch_lightning as pl\n",
    "from omegaconf import OmegaConf\n",
    "from nlp_engine.data.data_utils import DataUtils, create_train_test_data\n",
    "import time\n",
    "import onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "951dfae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.utils import logging\n",
    "from nemo.collections.nlp.parts.utils_funcs import tensor2list\n",
    "from nemo.collections.nlp.models.text_classification import TextClassificationModel\n",
    "from nemo.collections.nlp.data.text_classification import TextClassificationDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1059b586",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NemoTrainer(object):\n",
    "    def __init__(self):\n",
    "       \n",
    "        config_file = \"/config/intent_slot_classification_config.yaml\"\n",
    "        #print(config_file)\n",
    "        self.config = OmegaConf.load(config_file)\n",
    "        print(OmegaConf.to_yaml(self.config))\n",
    "        self.config.model.data_dir = '/nemo_format/'\n",
    "        \n",
    "        # lets modify some trainer configs\n",
    "        # checks if we have GPU available and uses it\n",
    "        accelerator = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
    "        self.config.trainer.devices = 1\n",
    "        self.config.trainer.accelerator = accelerator\n",
    "\n",
    "        self.config.trainer.precision = 16 if torch.cuda.is_available() else 32\n",
    "\n",
    "        # for mixed precision training, uncomment the line below (precision should be set to 16 and amp_level to O1):\n",
    "        # config.trainer.amp_level = O1\n",
    "\n",
    "        # remove distributed training flags\n",
    "        self.config.trainer.strategy = None\n",
    "\n",
    "        # setup a small number of epochs for demonstration purposes of this tutorial\n",
    "        self.config.trainer.max_epochs = 5\n",
    "\n",
    "        self.trainer = pl.Trainer(**self.config.trainer)\n",
    "\n",
    "        exp_dir = exp_manager(self.trainer, self.config.get(\"exp_manager\", None))\n",
    "        # the exp_dir provides a path to the current experiment for easy access\n",
    "        print(str(exp_dir))\n",
    "     \n",
    "    def train(self):\n",
    "        # initialize the model\n",
    "        model = nemo_nlp.models.IntentSlotClassificationModel(self.config.model, trainer=self.trainer)\n",
    "\n",
    "        # train\n",
    "        self.trainer.fit(model)\n",
    "\n",
    "    def test(self):\n",
    "        # specify checkpoint path with .nemo file\n",
    "        checkpoint_path = os.path.join(\"/nemo_experiments/IntentSlot/2023-05-22_11-13-36/checkpoints/\", \"IntentSlot.nemo\")\n",
    "\n",
    "        # load the model from this checkpoint\n",
    "        eval_model = nemo_nlp.models.IntentSlotClassificationModel.restore_from(checkpoint_path)\n",
    "        #eval_model.optimize_threshold(self.config.model.test_ds, 'dev')\n",
    "        queries = []\n",
    "        \n",
    "        test_generation_file = \"intent_detection_test_sentences.txt\"\n",
    "        with open(test_generation_file, \"r\", encoding=\"utf-8\") as r:\n",
    "            for line in r:\n",
    "                queries.append(line)\n",
    "\n",
    "        # We use the optimized threshold for predictions\n",
    "        pred_intents, pred_slots = eval_model.predict_from_examples(queries, self.config.model.test_ds)\n",
    "        logging.info('The prediction results of some sample queries with the trained model:')\n",
    "\n",
    "        for query, intent, slots in zip(queries, pred_intents, pred_slots):\n",
    "            logging.info(f'Query : {query}')\n",
    "            logging.info(f'Predicted Intents: {intent}')\n",
    "            logging.info(f'Predicted Slots: {slots}')\n",
    "            \n",
    "    def export_model(self, nemo_checkpoint_path, onnx_filename):\n",
    "        # extract the path of the best checkpoint from the training, you may update it to any other saved checkpoint file\n",
    "        #checkpoint_path = self.trainer.checkpoint_callback.best_model_path\n",
    "        checkpoint_path = os.path.join(nemo_checkpoint_path, \"IntentSlot.nemo\")\n",
    "        # load the model from this checkpoint\n",
    "        eval_model = nemo_nlp.models.IntentSlotClassificationModel.restore_from(checkpoint_path)\n",
    "        eval_model.eval()\n",
    "  \n",
    "        eval_model.export(output=onnx_filename, onnx_opset_version=14)\n",
    "        \n",
    "    \n",
    "    def to_numpy(self, tensor):\n",
    "        return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "    def create_infer_dataloader(self, model, queries):\n",
    "        batch_size = len(queries)\n",
    "  \n",
    "        dataset = TextClassificationDataset(tokenizer=model.tokenizer, queries=queries, max_seq_length=50)\n",
    "\n",
    "        return torch.utils.data.DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=2,\n",
    "            pin_memory=True,\n",
    "            drop_last=False,\n",
    "            collate_fn=dataset.collate_fn,\n",
    "        )\n",
    "    def postprocessing(self, intent_preds, slot_preds, intent_label_map, slot_label_map):\n",
    "        processed_results = []\n",
    "        for intent_pred, slot_pred in zip(intent_preds, slot_preds):\n",
    "            intent_label = intent_label_map[intent_pred]\n",
    "            slot_labels = [slot_label_map[slot] for slot in slot_pred]\n",
    "            processed_results.append((intent_label, slot_labels))\n",
    "        return processed_results\n",
    "        \n",
    "    def inference(self, queries, nemo_checkpoint_path, onnx_model, intent_slot_label_path):\n",
    "        checkpoint_path = os.path.join(nemo_checkpoint_path, \"IntentSlot.nemo\")\n",
    "        # load the model from this checkpoint\n",
    "        eval_model = nemo_nlp.models.IntentSlotClassificationModel.restore_from(checkpoint_path)\n",
    "        eval_model.eval()\n",
    "        \n",
    "        \n",
    "        start_time = time.time()\n",
    "        infer_datalayer = self.create_infer_dataloader(eval_model, queries)\n",
    "\n",
    "        ort_session = onnxruntime.InferenceSession(onnx_model)\n",
    "        \n",
    "        \n",
    "        for batch in infer_datalayer:\n",
    "            input_ids, input_type_ids, input_mask, subtokens_mask = batch\n",
    "            ort_inputs = {ort_session.get_inputs()[0].name: self.to_numpy(input_ids),\n",
    "                          ort_session.get_inputs()[1].name: self.to_numpy(input_mask),\n",
    "                          ort_session.get_inputs()[2].name: self.to_numpy(input_type_ids),}\n",
    "            #ologits = ort_session.run(None, ort_inputs)\n",
    "            \n",
    "            intent_logits, slot_logits = ort_session.run(None, ort_inputs)\n",
    "\n",
    "            intent_preds = tensor2list(torch.argmax(torch.from_numpy(intent_logits), dim=-1))\n",
    "            slot_preds = tensor2list(torch.argmax(torch.from_numpy(slot_logits), dim=-1))\n",
    "            \n",
    "            \n",
    "            # Define intent label map\n",
    "            intent_label_map = {}\n",
    "            with open(intent_slot_label_path + 'dict.intents.csv', 'r') as file:\n",
    "                for index, line in enumerate(file):\n",
    "                    label = line.strip()\n",
    "                    intent_label_map[index] = label\n",
    "            # Define slot label map\n",
    "            slot_label_map = {}\n",
    "            with open(intent_slot_label_path + 'dict.slots.csv', 'r') as file:\n",
    "                for index, line in enumerate(file):\n",
    "                    label = line.strip()\n",
    "                    slot_label_map[index] = label\n",
    "    \n",
    "            \n",
    "            processed_results = self.postprocessing(intent_preds, slot_preds, intent_label_map, slot_label_map)\n",
    "\n",
    "            logging.info('The prediction results of some sample queries with the trained model:')\n",
    "            for query, (intent_result, slot_result) in zip(queries, processed_results):\n",
    "                logging.info(f'Query: {query}')\n",
    "                logging.info(f'Predicted intent: {intent_result}')\n",
    "                logging.info(f'Predicted slots: {slot_result}')\n",
    "            stop_time = time.time()\n",
    "            print(\"Inference time:\", stop_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada99962",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fee880",
   "metadata": {},
   "outputs": [],
   "source": [
    "nemo_model = NemoTrainer()\n",
    "nemo_model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40399c9e",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db36ed63",
   "metadata": {},
   "outputs": [],
   "source": [
    "nemo_model = NemoTrainer()\n",
    "nemo_model.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c59995",
   "metadata": {},
   "source": [
    "# Export model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e236218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/core/export.html\n",
    "# https://onnxruntime.ai/docs/reference/compatibility.html\n",
    "nemo_model = NemoTrainer()\n",
    "nemo_checkpoint_path = \"/nemo_experiments/IntentSlot/2023-05-29_10-51-01/checkpoints/\"\n",
    "onnx_filename = \"turkish_isc.onnx\"\n",
    "nemo_model.export_model(nemo_checkpoint_path, onnx_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b879bbb",
   "metadata": {},
   "source": [
    "# Onnxruntime Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b374605",
   "metadata": {},
   "outputs": [],
   "source": [
    "nemo_model = NemoTrainer()\n",
    "intent_slot_label_path = \"/nemo_format/\"\n",
    "nemo_checkpoint_path= \"/nemo_experiments/IntentSlot/2023-05-29_10-51-01/checkpoints/\"\n",
    "onnx_model = \"turkish_isc.onnx\"\n",
    "\n",
    "query = ['kahve pişirmeyi başlat']\n",
    "\n",
    "nemo_model.inference(query, nemo_checkpoint_path, onnx_model, intent_slot_label_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a560bce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35edb3d",
   "metadata": {},
   "source": [
    "# TensorRT Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8248752",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pycuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c24d932",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetson_voice_utils.trt_model import TRTModel\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class IntentSlotClassificationTRTInference(object):\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.nlp_dynamic_shapes = False\n",
    "        self.intent_slot_label_path = \"/nemo_format/\"\n",
    "        \n",
    "        queries = []\n",
    "\n",
    "        config_file = \"/intent_slot_classification_config.yaml\"\n",
    "        self.config = OmegaConf.load(config_file)\n",
    "        \n",
    "        self.config.model_path = 'turkish_isc.onnx'\n",
    "        print(OmegaConf.to_yaml(self.config))\n",
    "             \n",
    "        # load model\n",
    "        dynamic_shapes = {'max' : (1, self.config.model['language_model']['max_seq_length'])}  # (batch_size, sequence_length)\n",
    "        \n",
    "        \n",
    "        if self.nlp_dynamic_shapes:\n",
    "            dynamic_shapes['min'] = (1, 1)\n",
    "        #print(dynamic_shapes)\n",
    "        \n",
    "        self.model = TRTModel(self.config, dynamic_shapes)\n",
    "        \n",
    "        # create tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.config.model['tokenizer']['tokenizer_name'])\n",
    "             \n",
    "        \n",
    "    def normalize_logits(self, logits):\n",
    "        \"\"\"\n",
    "        Normalize logits such that they are distributed between [0,1]\n",
    "        \"\"\"\n",
    "        return np.exp(logits - np.log(np.sum(np.exp(logits), axis=-1, keepdims=True)))\n",
    "    \n",
    "    def intent_labels(self):\n",
    "        \"\"\"\n",
    "        List of the intent class labels.\n",
    "        \"\"\"\n",
    "        # Define intent label map\n",
    "        intent_labels = []\n",
    "        with open(self.intent_slot_label_path + 'dict.intents.csv', 'r') as file:\n",
    "            for intent in file:\n",
    "                intent_labels.append(intent.strip())     \n",
    "               \n",
    "        return intent_labels\n",
    "\n",
    "\n",
    "    def intent_label(self, index):\n",
    "        \"\"\"\n",
    "        Return an intent label by index (with bounds checking) \n",
    "        \"\"\"\n",
    "        return self.intent_labels()[int(index)] if index < len(self.intent_labels()) else 'Unknown_Intent'\n",
    "    \n",
    "    def slot_labels(self):\n",
    "        \"\"\"\n",
    "        List of the slot class labels.\n",
    "        \"\"\"\n",
    "        # Define slot label map\n",
    "        slot_labels = []\n",
    "        with open(self.intent_slot_label_path + 'dict.slots.csv', 'r') as file:\n",
    "            for slot in file:\n",
    "                slot_labels.append(slot.strip())\n",
    "  \n",
    "        return slot_labels \n",
    "    \n",
    "    def slot_label(self, index):\n",
    "        \"\"\"\n",
    "        Return a slot label by index (with bounds checking)\n",
    "        \"\"\"\n",
    "        return self.slot_labels()[int(index)] if index < len(self.slot_labels()) else self.null_slot\n",
    "   \n",
    "        \n",
    "    def find_subtokens(self, encodings, method='char_span'):\n",
    "        \"\"\"\n",
    "        Compute the subtoken mask, where each token is marked as True if it's a subtoken or False otherwise.\n",
    "        Longer words/acronyms may be tokenized into mulitple word pieces (called subtokens), for example:\n",
    "\n",
    "            'Yosemite' -> ['yo', '##se', '##mite']\n",
    "            'U.S.' -> ['u', '.', 's', '.']\n",
    "\n",
    "        Parameters:\n",
    "          encodings (BatchEncoding) -- Output from tokenizer\n",
    "\n",
    "          method (string) -- If 'char_span', the subtoken mask will be determined by looking at the character\n",
    "                             indices.  Tokens that map to characters that are side-by-side are flagged as subtokens.\n",
    "\n",
    "                             If 'subtoken_delimiters', subtokens will be identified by looking for '##' symbols.\n",
    "                             However this can miss punctuated subtokens, such as 'U.S.'\n",
    "\n",
    "        Returns boolean subtoken mask array with shape (num_queries, num_tokens)\n",
    "        \"\"\"\n",
    "        num_queries = encodings['input_ids'].shape[0]\n",
    "        subtoken_mask = []\n",
    "\n",
    "        if method == 'char_span':\n",
    "            for query_idx in range(num_queries):\n",
    "                mask = []\n",
    "                last_char = -1\n",
    "                tokens = encodings.tokens(query_idx)\n",
    "\n",
    "                for token_idx, word_id in enumerate(encodings.word_ids(query_idx)):\n",
    "                    if word_id is None:  # skip special tokens\n",
    "                        mask.append(False)\n",
    "                        continue\n",
    "\n",
    "                    chars = encodings.token_to_chars(query_idx, token_idx)\n",
    "\n",
    "                    if chars[0] == last_char:\n",
    "                        mask.append(True)\n",
    "                    else:\n",
    "                        mask.append(False)\n",
    "\n",
    "                    last_char = chars[1]\n",
    "\n",
    "                subtoken_mask.append(mask)\n",
    "\n",
    "        elif method == 'subtoken_delimiters':\n",
    "            for query_idx in range(num_queries):\n",
    "                subtoken_mask.append([token.startswith('##') for token in encodings.tokens(query_idx)])\n",
    "        else:\n",
    "            raise ValueError(f\"invalid method ('{method}')\")\n",
    "\n",
    "        return np.asarray(subtoken_mask)\n",
    "        \n",
    "       \n",
    "     \n",
    "    def inference(self, queries):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        \"\"\"\n",
    "        Perform intent/slot classification on the input query.\n",
    "        \n",
    "        Parameters:\n",
    "          query (string) -- The text query, for example:\n",
    "                             'What is the weather in San Francisco tomorrow?'\n",
    "\n",
    "        Returns a dict with the following keys:\n",
    "             'intent' (string) -- the classified intent label\n",
    "             'score' (float) -- the intent probability [0,1]\n",
    "             'slots' (list[dict]) -- a list of dicts, where each dict has the following keys:\n",
    "                  'slot' (string) -- the slot label\n",
    "                  'text' (string) -- the slot text from the query\n",
    "                  'score' (float) -- the slot probability [0,1]\n",
    "        \"\"\"\n",
    "\n",
    "        queries = ['kahve pişirmeyi başlat']\n",
    "        \n",
    "        self.null_slot = self.slot_labels()[-1]  # 'O' in assistant dataset - always the last label?\n",
    "        \n",
    "        encodings = self.tokenizer(\n",
    "            text=query,\n",
    "            padding='longest' if self.nlp_dynamic_shapes else 'max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.config.model['language_model']['max_seq_length'],\n",
    "            return_tensors='np',\n",
    "            return_token_type_ids=True,\n",
    "            return_overflowing_tokens=True,\n",
    "            return_offsets_mapping=True,\n",
    "            return_special_tokens_mask=True,\n",
    "        )\n",
    "\n",
    "        # during slot classification, we want to ignore slots from subtokens and special tokens \n",
    "        subtoken_mask = self.find_subtokens(encodings, method='subtoken_delimiters')\n",
    "        ignore_mask = subtoken_mask | encodings['special_tokens_mask']\n",
    "        \n",
    "        \n",
    "        # retrieve the inputs from the encoded tokens\n",
    "        inputs = {}\n",
    "        \n",
    "        for input in self.model.inputs:\n",
    "            if input.name not in encodings:\n",
    "                raise ValueError(f\"the encoded inputs from the tokenizer doesn't contain '{input.name}'\")\n",
    "\n",
    "            inputs[input.name] = encodings[input.name]\n",
    "                    \n",
    "        # run the model\n",
    "        intent_logits, slot_logits = self.model.execute(inputs)\n",
    "        \n",
    "        intent_logits = self.normalize_logits(intent_logits)\n",
    "        slot_logits = self.normalize_logits(slot_logits)\n",
    "\n",
    "        intent_preds = np.argmax(intent_logits, axis=-1)\n",
    "        slot_preds = np.argmax(slot_logits, axis=-1)\n",
    "\n",
    "        # convert numerical outputs to intent/slot labels\n",
    "        results = []\n",
    "\n",
    "        for query_idx, intent_id in enumerate(intent_preds):\n",
    "            results.append({\n",
    "                'intent' : self.intent_label(intent_id),\n",
    "                'score' : intent_logits[query_idx][intent_id],\n",
    "                'slots' : []\n",
    "            })\n",
    "            \n",
    "        for query_idx, slots in enumerate(slot_preds):\n",
    "            query_slots = [self.slot_label(slot) for slot in slots]\n",
    "\n",
    "            for token_idx, slot in enumerate(query_slots):\n",
    "                # ignore unclassified slots or masked tokens\n",
    "                if slot == self.null_slot or ignore_mask[query_idx][token_idx]:\n",
    "                    continue\n",
    "                    \n",
    "                # convert from token index back to the query string\n",
    "                chars = encodings.token_to_chars(query_idx, token_idx)\n",
    "                text = query[chars[0]:chars[1]]      # queries[query_idx]\n",
    "                \n",
    "                # append subtokens from the query to the text\n",
    "                for subtoken_idx in range(token_idx+1, len(query_slots)):\n",
    "                    if subtoken_mask[query_idx][subtoken_idx]:\n",
    "                        subtoken_chars = encodings.token_to_chars(query_idx, subtoken_idx)\n",
    "                        text += query[subtoken_chars[0]:subtoken_chars[1]]\n",
    "                    else:\n",
    "                        break\n",
    "                        \n",
    "                results[query_idx]['slots'].append({\n",
    "                    'slot' : slot,\n",
    "                    'text' : text,\n",
    "                    'score' : slot_logits[query_idx][token_idx][slots[token_idx]]\n",
    "                })\n",
    "        print(\"TRT inference time:\", time.time() - start_time)\n",
    "\n",
    "        if len(results) == 1:\n",
    "            return results[0]\n",
    "        else:\n",
    "            return results\n",
    "        \n",
    "trt_nemo = IntentSlotClassificationTRTInference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d00950",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ['iki kişilik türk kahvesi yap']\n",
    "trt_nemo.inference(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torcht2s",
   "language": "python",
   "name": "torcht2s"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
